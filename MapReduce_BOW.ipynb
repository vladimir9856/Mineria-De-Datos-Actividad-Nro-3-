{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MapReduce BOW.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CORRESPONDIENTE A LA 2DA PARCIAL - MapReduce BOW"
      ],
      "metadata": {
        "id": "whoq9SZLOu81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Universidad Nacional de San Antonio Abad del Cusco\n",
        "\n",
        "Asignatura: Mineria de Datos\n",
        "\n",
        "Docente   : Carlos Fernando Montoya Cubas\n",
        "\n",
        "Estudiante     : Vladimir Dante Casilla Percca\n",
        "\n"
      ],
      "metadata": {
        "id": "dPW5fRtQOxSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instalacion de pySpark en Google Colaboratory"
      ],
      "metadata": {
        "id": "8nQDEw2ROzk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.0.1 py4j==0.10.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhKGbT-fO1Fk",
        "outputId": "c4ebe595-0dda-4ec6-9957-df40f592ac0a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.0.1\n",
            "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 57.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612246 sha256=b55491133248d83aed9afe455b15767ab742be7e80ff679afc1865d953dadfa8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inicializar variables de entorno"
      ],
      "metadata": {
        "id": "mJmBPUIaPAHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "5_4uGTH9PA5O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mostrar version de entornos spark"
      ],
      "metadata": {
        "id": "nvgNVf68PDNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "w5ZHePZ8PGgZ",
        "outputId": "b0dfe46e-a5f0-4e07-b0fd-d78cf4f2825a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://89c882bbb6a8:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bag Of Words (Bow)"
      ],
      "metadata": {
        "id": "XKHzwJNnPIUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es un método que se utiliza en el procesado del lenguaje para representar documentos ignorando el orden de las palabras. En este modelo, cada documento parece una bolsa que contiene algunas palabras. "
      ],
      "metadata": {
        "id": "soGFr7fWPJ86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "data=[(1,'Los titanes no son nuestros únicos enemigos'),\n",
        "      (2,\"Nadie puede predecir el resultado, cada decisión que tomas tiene un significado sólo para afectar tu próxima decisión\"),\n",
        "      (3,\"Parecía que tenias un sueño profundo, pero parece que sigues soñando despierto\")]\n",
        "#creacion para el tokenizer o bag of words\n",
        "lines=sc.parallelize(data)\n",
        "lines.collect()\n",
        "map1=lines.flatMap(lambda x: [((x[0],i),1) for i in x[1].split()])\n",
        "map1.collect()\n",
        "#utilizacion del map reduce\n",
        "reduce=map1.reduceByKey(lambda x,y:x+y)\n",
        "#imprimir el baw of word de las 3 oraciones\n",
        "reduce.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8Y4ttv3PLaZ",
        "outputId": "4fc0877a-d297-4563-9cb1-75ab89e50988"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((1, 'Los'), 1),\n",
              " ((1, 'nuestros'), 1),\n",
              " ((1, 'únicos'), 1),\n",
              " ((1, 'enemigos'), 1),\n",
              " ((2, 'predecir'), 1),\n",
              " ((2, 'el'), 1),\n",
              " ((2, 'cada'), 1),\n",
              " ((2, 'decisión'), 2),\n",
              " ((2, 'que'), 1),\n",
              " ((2, 'tomas'), 1),\n",
              " ((2, 'significado'), 1),\n",
              " ((2, 'para'), 1),\n",
              " ((2, 'próxima'), 1),\n",
              " ((3, 'Parecía'), 1),\n",
              " ((3, 'un'), 1),\n",
              " ((3, 'profundo,'), 1),\n",
              " ((3, 'sigues'), 1),\n",
              " ((3, 'despierto'), 1),\n",
              " ((1, 'titanes'), 1),\n",
              " ((1, 'no'), 1),\n",
              " ((1, 'son'), 1),\n",
              " ((2, 'Nadie'), 1),\n",
              " ((2, 'puede'), 1),\n",
              " ((2, 'resultado,'), 1),\n",
              " ((2, 'tiene'), 1),\n",
              " ((2, 'un'), 1),\n",
              " ((2, 'sólo'), 1),\n",
              " ((2, 'afectar'), 1),\n",
              " ((2, 'tu'), 1),\n",
              " ((3, 'que'), 2),\n",
              " ((3, 'tenias'), 1),\n",
              " ((3, 'sueño'), 1),\n",
              " ((3, 'pero'), 1),\n",
              " ((3, 'parece'), 1),\n",
              " ((3, 'soñando'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aplicacion TF- IDF"
      ],
      "metadata": {
        "id": "fJqhNYiRPNAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculamos TF\n",
        "tf=reduce.map(lambda x: (x[0][1],(x[0][0],x[1])))\n",
        "tf.collect()\n",
        "map3=reduce.map(lambda x: (x[0][1],(x[0][0],x[1],1)))\n",
        "map3.collect()\n",
        "map4=map3.map(lambda x:(x[0],x[1][2]))\n",
        "map4.collect()\n",
        "reduce2=map4.reduceByKey(lambda x,y:x+y)\n",
        "reduce2.collect()\n",
        "#Calculamos IDF\n",
        "idf=reduce2.map(lambda x: (x[0],math.log10(len(data)/x[1])))\n",
        "idf.collect()\n",
        "rdd=tf.join(idf)\n",
        "rdd.collect()\n",
        "#calculamos TF-IDF\n",
        "rdd=rdd.map(lambda x: (x[1][0][0],(x[0],x[1][0][1],x[1][1],x[1][0][1]*x[1][1]))).sortByKey()"
      ],
      "metadata": {
        "id": "zbfqNGK8POx7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mostrar Tabla TF-IDF"
      ],
      "metadata": {
        "id": "Q6zmuN4sPPtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "park = SparkSession(sc)\n",
        "rdd=rdd.map(lambda x: (x[0],x[1][0],x[1][1],x[1][2]))\n",
        "hasattr(rdd, \"toDF\")\n",
        "rdd.toDF([\"ID\",\"Token\",\"Bag_of_Word\",\"TF-IDF\"]).show(50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIOFl5uHPQOk",
        "outputId": "7636a462-2c37-466e-b4c0-f2c9c9bfe789"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+-----------+-------------------+\n",
            "| ID|      Token|Bag_of_Word|             TF-IDF|\n",
            "+---+-----------+-----------+-------------------+\n",
            "|  1|    titanes|          1|0.47712125471966244|\n",
            "|  1|         no|          1|0.47712125471966244|\n",
            "|  1|        son|          1|0.47712125471966244|\n",
            "|  1|        Los|          1|0.47712125471966244|\n",
            "|  1|   nuestros|          1|0.47712125471966244|\n",
            "|  1|     únicos|          1|0.47712125471966244|\n",
            "|  1|   enemigos|          1|0.47712125471966244|\n",
            "|  2|   predecir|          1|0.47712125471966244|\n",
            "|  2|         el|          1|0.47712125471966244|\n",
            "|  2|      tomas|          1|0.47712125471966244|\n",
            "|  2|significado|          1|0.47712125471966244|\n",
            "|  2|      puede|          1|0.47712125471966244|\n",
            "|  2| resultado,|          1|0.47712125471966244|\n",
            "|  2|      tiene|          1|0.47712125471966244|\n",
            "|  2|         tu|          1|0.47712125471966244|\n",
            "|  2|       cada|          1|0.47712125471966244|\n",
            "|  2|   decisión|          2|0.47712125471966244|\n",
            "|  2|        que|          1|0.17609125905568124|\n",
            "|  2|       para|          1|0.47712125471966244|\n",
            "|  2|    próxima|          1|0.47712125471966244|\n",
            "|  2|         un|          1|0.17609125905568124|\n",
            "|  2|      Nadie|          1|0.47712125471966244|\n",
            "|  2|       sólo|          1|0.47712125471966244|\n",
            "|  2|    afectar|          1|0.47712125471966244|\n",
            "|  3|     parece|          1|0.47712125471966244|\n",
            "|  3|  despierto|          1|0.47712125471966244|\n",
            "|  3|        que|          2|0.17609125905568124|\n",
            "|  3|     tenias|          1|0.47712125471966244|\n",
            "|  3|      sueño|          1|0.47712125471966244|\n",
            "|  3|       pero|          1|0.47712125471966244|\n",
            "|  3|    soñando|          1|0.47712125471966244|\n",
            "|  3|    Parecía|          1|0.47712125471966244|\n",
            "|  3|         un|          1|0.17609125905568124|\n",
            "|  3|  profundo,|          1|0.47712125471966244|\n",
            "|  3|     sigues|          1|0.47712125471966244|\n",
            "+---+-----------+-----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}