{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5 ALGORITMOS DE PREPROCESAMIENTO.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CORRESPONDIENTE A LA 2DA PARCIAL - 5 ALGORITMOS DE PREPROCESAMIENTO"
      ],
      "metadata": {
        "id": "0C_f3T6oP4J1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Universidad Nacional de San Antonio Abad del Cusco\n",
        "\n",
        "Asignatura: Mineria de Datos\n",
        "\n",
        "Docente   : Carlos Fernando Montoya Cubas\n",
        "\n",
        "Estudiante    : Vladimir Dante Casilla Percca\n"
      ],
      "metadata": {
        "id": "l1i5BxdZP57j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instalacion de pySpark en Google Colaboratory"
      ],
      "metadata": {
        "id": "Fv8gtYvvP7B1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.0.1 py4j==0.10.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paTsqIDFP78p",
        "outputId": "815aa441-9274-4b3c-dcf6-8d6dd99b02ac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.0.1\n",
            "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2 MB 31 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 66.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612246 sha256=9137f9725c73e849c5a3fa9eddc4a8d8fca39890a7339ff18f59f2bb8636acc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "wLeMy8K7P9HN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Escalonamiento"
      ],
      "metadata": {
        "id": "-0BuRDQNP-AR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los algoritmos basados en metodos de gradiente tienden a beneficiarse cuando los atributos estan entre ."
      ],
      "metadata": {
        "id": "cbivMo3BP_uN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from pyspark import SparkConf \n",
        "from pyspark.context import SparkContext \n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType"
      ],
      "metadata": {
        "id": "mw69IdI5QBKU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "E3tqsV8LQDUf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos datos\n",
        "df = spark.createDataFrame([ (1, 'Aldo',12560,45,\"True\"),\n",
        "                             (2, 'Benzema',42560,90,\"False\"),\n",
        "                             (3, 'Cesar',31285,81,\"True\"),\n",
        "                             (4, 'Doris',25285,50,\"True\"),\n",
        "                             (5, 'Etson',32285,55,\"False\"),\n",
        "                             (6, 'Fausto',18085,61,\"True\"),\n",
        "                             (7, 'Gilberto',52185,70,\"False\"),\n",
        "                             (8, 'Humberto',10345,46,\"False\")\n",
        "                           ], [\"id_usuario\", \"Nombre\",\"Ingresos\",\"Por_dia\",\"Morocidad\"])\n",
        "\n",
        "#Mostrar datos iniciales\n",
        "print(\"Data :\")\n",
        "df.show(8)\n",
        "\n",
        "#UDF para convertir el tipo de columna de vector a tipo doble\n",
        "unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
        "\n",
        "#Iterando sobre columnas para el escalonamiento\n",
        "for i in [\"Ingresos\",\"Por_dia\"]:\n",
        "    #Transformación con VectorAssembler (conversión de columna a tipo vector)\n",
        "    assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
        "\n",
        "    #Transformación MinMaxScaler\n",
        "    scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Escalonado\")\n",
        "\n",
        "    #Canalización de VectorAssembler y MinMaxScaler\n",
        "    pipeline = Pipeline(stages=[assembler, scaler])\n",
        "\n",
        "    #Ajuste del pipeline en el marco de datos\n",
        "    df = pipeline.fit(df).transform(df).withColumn(i+\"_Escalonado\", unlist(i+\"_Escalonado\")).drop(i+\"_Vect\")\n",
        "\n",
        "print(\"Data + Escalonamiento :\")\n",
        "df.show(8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k01RCxQJQF2l",
        "outputId": "ef2b19a3-91d7-4fa2-b039-292290feb50f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data :\n",
            "+----------+--------+--------+-------+---------+\n",
            "|id_usuario|  Nombre|Ingresos|Por_dia|Morocidad|\n",
            "+----------+--------+--------+-------+---------+\n",
            "|         1|    Aldo|   12560|     45|     True|\n",
            "|         2| Benzema|   42560|     90|    False|\n",
            "|         3|   Cesar|   31285|     81|     True|\n",
            "|         4|   Doris|   25285|     50|     True|\n",
            "|         5|   Etson|   32285|     55|    False|\n",
            "|         6|  Fausto|   18085|     61|     True|\n",
            "|         7|Gilberto|   52185|     70|    False|\n",
            "|         8|Humberto|   10345|     46|    False|\n",
            "+----------+--------+--------+-------+---------+\n",
            "\n",
            "Data + Escalonamiento :\n",
            "+----------+--------+--------+-------+---------+-------------------+------------------+\n",
            "|id_usuario|  Nombre|Ingresos|Por_dia|Morocidad|Ingresos_Escalonado|Por_dia_Escalonado|\n",
            "+----------+--------+--------+-------+---------+-------------------+------------------+\n",
            "|         1|    Aldo|   12560|     45|     True|              0.053|               0.0|\n",
            "|         2| Benzema|   42560|     90|    False|               0.77|               1.0|\n",
            "|         3|   Cesar|   31285|     81|     True|                0.5|               0.8|\n",
            "|         4|   Doris|   25285|     50|     True|              0.357|             0.111|\n",
            "|         5|   Etson|   32285|     55|    False|              0.524|             0.222|\n",
            "|         6|  Fausto|   18085|     61|     True|              0.185|             0.356|\n",
            "|         7|Gilberto|   52185|     70|    False|                1.0|             0.556|\n",
            "|         8|Humberto|   10345|     46|    False|                0.0|             0.022|\n",
            "+----------+--------+--------+-------+---------+-------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El valor minimo de la columna Ingresos es 10345 y el maximo es 52185, estos estan representados correctamente en el escalonamiento teniendo un 0 y 1 respectivamente, de manera similar sucede en la columna Por_dia."
      ],
      "metadata": {
        "id": "B-ku6S3wQi6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Normalizacion\n",
        "Podemos normalizar cada muestra del dataset usando la normalización vectorial:"
      ],
      "metadata": {
        "id": "yenjoyoVQjuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql.types import FloatType\n",
        "#Nomazlizar elementos aleatorios \n",
        "def normalizar(x):\n",
        "    #Vector para normalizar\n",
        "    listaCuadrada=x.map(lambda xi:xi*xi)\n",
        "    total=listaCuadrada.sum()\n",
        "    val=math.sqrt(total)\n",
        "    #Escalonar vector\n",
        "    lista= x.map(lambda xi :(xi/val))\n",
        "    return lista \n",
        "\n",
        "#Creamos datos\n",
        "df = spark.createDataFrame([ (1, 'Aldo',12560,45,\"True\"),\n",
        "                             (2, 'Benzema',42560,90,\"False\"),\n",
        "                             (3, 'Cesar',31285,81,\"True\"),\n",
        "                             (4, 'Doris',25285,50,\"True\"),\n",
        "                             (5, 'Etson',32285,55,\"False\"),\n",
        "                             (6, 'Fausto',18085,61,\"True\"),\n",
        "                             (7, 'Gilberto',52185,70,\"False\"),\n",
        "                             (8, 'Humberto',10345,46,\"False\")\n",
        "                           ], [\"id_usuario\", \"Nombre\",\"Ingresos\",\"Por_dia\",\"Morocidad\"])\n",
        "\n",
        "#Mostrar datos iniciales\n",
        "print(\"Data :\")\n",
        "df.show(8)\n",
        "\n",
        "\n",
        "lista = df.select('Ingresos').rdd.map(lambda row : row[0]).collect()\n",
        "Vector = sc.parallelize(lista,4)\n",
        "VectorEscalonada=normalizar(Vector)\n",
        "a=VectorEscalonada.collect()\n",
        "df2 = spark.createDataFrame(a, FloatType())\n",
        "df2=df2.withColumnRenamed(\"value\", \"Ingresos_Normalizado\")\n",
        "print(\"Columna Ingresos Normalizado :\")\n",
        "#Mostrar la columna\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_96ytzANQlO9",
        "outputId": "332a3651-aada-4b82-a3e7-5673dc025d5d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data :\n",
            "+----------+--------+--------+-------+---------+\n",
            "|id_usuario|  Nombre|Ingresos|Por_dia|Morocidad|\n",
            "+----------+--------+--------+-------+---------+\n",
            "|         1|    Aldo|   12560|     45|     True|\n",
            "|         2| Benzema|   42560|     90|    False|\n",
            "|         3|   Cesar|   31285|     81|     True|\n",
            "|         4|   Doris|   25285|     50|     True|\n",
            "|         5|   Etson|   32285|     55|    False|\n",
            "|         6|  Fausto|   18085|     61|     True|\n",
            "|         7|Gilberto|   52185|     70|    False|\n",
            "|         8|Humberto|   10345|     46|    False|\n",
            "+----------+--------+--------+-------+---------+\n",
            "\n",
            "Columna Ingresos Normalizado :\n",
            "+--------------------+\n",
            "|Ingresos_Normalizado|\n",
            "+--------------------+\n",
            "|          0.14233384|\n",
            "|          0.48230317|\n",
            "|          0.35453135|\n",
            "|           0.2865375|\n",
            "|          0.36586368|\n",
            "|          0.20494485|\n",
            "|          0.59137666|\n",
            "|          0.11723276|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Binarizacion\n",
        "\n",
        "Consiste en transformar las cadenas de texto que tienen valores de string (True y False) a valores numericos, 0 para False y 1 para True"
      ],
      "metadata": {
        "id": "EXqt6g8wQpBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from functools import reduce"
      ],
      "metadata": {
        "id": "pGmnkzELQrMO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos el data a usar\n",
        "df = spark.createDataFrame([ (1, 'Aldo',12560,45,\"True\"),\n",
        "                             (2, 'Benzema',42560,90,\"False\"),\n",
        "                             (3, 'Cesar',31285,81,\"True\"),\n",
        "                             (4, 'Doris',25285,50,\"True\"),\n",
        "                             (5, 'Etson',32285,55,\"False\"),\n",
        "                             (6, 'Fausto',18085,61,\"True\"),\n",
        "                             (7, 'Gilberto',52185,70,\"False\"),\n",
        "                             (8, 'Humberto',10345,46,\"False\")\n",
        "                           ], [\"id_usuario\", \"Nombre\",\"Ingresos\",\"Por_dia\",\"Morocidad\"])\n",
        "\n",
        "#Mostramos los datos\n",
        "print(\"Data :\")\n",
        "df.show()\n",
        "\n",
        "#Aqui podemos elegir en que columnas aplicar la binarizacion\n",
        "cols = ['Morocidad']\n",
        "\n",
        "#Aplicar la binarizacion al dataframe spark\n",
        "df_reduced = reduce(lambda df, c: df.withColumn(c, F.when(df[c] == 'False', 0.0).otherwise(1.0)), cols, df)\n",
        "\n",
        "#Mostrar columna binarizada\n",
        "print(\"Data + Columna binarizada:\")\n",
        "df_reduced.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUaf0DnSQrug",
        "outputId": "eb200981-5839-4791-a6a9-d102fdc7023f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data :\n",
            "+----------+--------+--------+-------+---------+\n",
            "|id_usuario|  Nombre|Ingresos|Por_dia|Morocidad|\n",
            "+----------+--------+--------+-------+---------+\n",
            "|         1|    Aldo|   12560|     45|     True|\n",
            "|         2| Benzema|   42560|     90|    False|\n",
            "|         3|   Cesar|   31285|     81|     True|\n",
            "|         4|   Doris|   25285|     50|     True|\n",
            "|         5|   Etson|   32285|     55|    False|\n",
            "|         6|  Fausto|   18085|     61|     True|\n",
            "|         7|Gilberto|   52185|     70|    False|\n",
            "|         8|Humberto|   10345|     46|    False|\n",
            "+----------+--------+--------+-------+---------+\n",
            "\n",
            "Data + Columna binarizada:\n",
            "+----------+--------+--------+-------+---------+\n",
            "|id_usuario|  Nombre|Ingresos|Por_dia|Morocidad|\n",
            "+----------+--------+--------+-------+---------+\n",
            "|         1|    Aldo|   12560|     45|      1.0|\n",
            "|         2| Benzema|   42560|     90|      0.0|\n",
            "|         3|   Cesar|   31285|     81|      1.0|\n",
            "|         4|   Doris|   25285|     50|      1.0|\n",
            "|         5|   Etson|   32285|     55|      0.0|\n",
            "|         6|  Fausto|   18085|     61|      1.0|\n",
            "|         7|Gilberto|   52185|     70|      0.0|\n",
            "|         8|Humberto|   10345|     46|      0.0|\n",
            "+----------+--------+--------+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Distancia de Minkowski\n",
        "Como sabemos la distancia de minkowski es la Distancia entre dos vectores numéricos.\n",
        "\n",
        "El parámetro p de la métrica de distancia de Minkowski de Pyspark representa el orden de la norma. Cuando el orden (p) es 1, representará la Distancia de Manhattan y cuando el orden en la fórmula anterior es 2, representará la Distancia euclidiana."
      ],
      "metadata": {
        "id": "sGgGvv_nQu6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Creemos una función rNorm que toma como parámetro y devuelve una función que calcula el pNorm\n",
        "def pNorm(p):\n",
        "    def Dist(x,y):\n",
        "        return np.power(np.power(np.abs(x-y),p).sum(),1/float(p))\n",
        "    return Dist\n",
        "# Creemos un RDD con valores numéricos.\n",
        "np.random.seed(50)\n",
        "num_p = sc.parallelize(enumerate(np.random.random(size=(10,100))))"
      ],
      "metadata": {
        "id": "OK8w1aaGQxMv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To83Y5xCQykg",
        "outputId": "cf91fa86-b81b-4328-bb16-e419a61cd881"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[58] at readRDDFromFile at PythonRDD.scala:262"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Formula Minswoski\n",
        "dat_p = num_p.cartesian(num_p)\n",
        "dato_p = dat_p.map(lambda x: ((x[0][0],x[1][0]), (x[0][1],x[1][1])))\n",
        "#minwoski el valor p tendra diferentes valores \n",
        "p = 5\n",
        "#p = 6\n",
        "# p = 7\n",
        "Minkow = pNorm(p)\n",
        "dist = dato_p.map(lambda x: (x[0], Minkow(x[1][0],x[1][1])))\n",
        "soluc = dist.map(lambda x: x[1])\n",
        "minv, maxv, meanv = soluc.min(), soluc.max(), soluc.mean()\n",
        "print('minimo valor minkowski: ',minv)\n",
        "print('maximo valor minkowski: ',maxv)\n",
        "print('Media de los valores minkowski: ',meanv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzzwYQg3Q1Ux",
        "outputId": "3dfbbad8-5901-41c8-80c5-6e8341f6e9c7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimo valor minkowski:  0.0\n",
            "maximo valor minkowski:  1.444514010466728\n",
            "Media de los valores minkowski:  1.2070128755149965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Distancia Jaccard\n",
        "La distancia de Jaccard ( ) o coeficiente de Jaccard () mide el grado de similitud entre dos conjuntos, sea cual sea el tipo de elementos."
      ],
      "metadata": {
        "id": "7rUI9HP8Q3I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calcula la distancia Jaccard entre dos vectores binarios.\n",
        "#x, y (np.array): Matriz de enteros binarios x and y.\n",
        "#Returns: J (int): La distancia Jaccard entre x and y.\n",
        "def Jaccard(x,y):\n",
        "    return (x==y).sum()/float( np.maximum(x,y).sum())\n",
        "\n",
        "colores = sc.parallelize(enumerate([['rojo', 'negro', 'azul'],\n",
        "                             ['rosado', 'negro', 'verde'],\n",
        "                             ['rosado', 'amarillo', 'azul'],\n",
        "                             ['rosado', 'negro', 'verde'],\n",
        "                             ['rojo', 'amarillo', 'verde'],\n",
        "                            ]))\n",
        "datos = (colores\n",
        "             .flatMap(lambda x: [((x[0],xi),1) for xi in x[1]])\n",
        "             .reduceByKey(lambda x,y: x)\n",
        "             .map(lambda x: x[0])\n",
        "             )\n",
        "\n",
        "dato = dict((v,k) for k,v in datos.collect())\n",
        "ndato = len(dato)\n",
        "print(dato, ndato)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY8O2oPJQ7ky",
        "outputId": "7a292c41-6694-481f-e30c-25e46a866681"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rojo': 4, 'negro': 3, 'rosado': 3, 'verde': 3, 'azul': 2, 'amarillo': 4} 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Binarizar el vector categórico usando un diccionario de key\n",
        "#atributos (lista): Lista de atributos de un objeto dado\n",
        "def Bina(atributos,dato):  \n",
        "    array = np.zeros(len(dato))\n",
        "    for atr in atributos:\n",
        "        array[ dato[atr] ] = 1\n",
        "    return array\n",
        "\n",
        "# Convierta datosa formato binario usando key  dict\n",
        "binarizar = colores.map(lambda rec: (rec[0],Bina(rec[1], dato)))\n",
        "binarizar.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdF0KqR4Q80Z",
        "outputId": "36d74a4b-19f1-4819-c48e-1671f3514cf5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, array([0., 0., 1., 1., 1., 0.])),\n",
              " (1, array([0., 0., 0., 1., 0., 0.])),\n",
              " (2, array([0., 0., 1., 1., 1., 0.])),\n",
              " (3, array([0., 0., 0., 1., 0., 0.])),\n",
              " (4, array([0., 0., 0., 1., 1., 0.]))]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adquirir dentro de los  PySpark para hallar produto cartesiano \n",
        "Binario = binarizar.cartesian(binarizar)\n",
        "# Aplicar un mapa para transformar nuestro RDD en un RDD de tupla ((id1, id2), (vector1, vector2))\n",
        "# use el comando take (1) e imprima el resultado para verificar el formato RDD actual\n",
        "Binario_par = Binario.map(lambda x: ((x[0][0],x[1][0]), (x[0][1],x[1][1])))\n",
        "#Aplicar un mapa para calcular la distancia de Jaccard entre pares\n",
        "jacRDD = Binario_par.map(lambda x: (x[0], Jaccard(x[1][0],x[1][1])))\n",
        "#calcular min, max, mean\n",
        "statJRDD = jacRDD.map(lambda x: x[1])\n",
        "Jmin, Jmax, Jmean = statJRDD.min(), statJRDD.max(), statJRDD.mean()\n",
        "print (\"\\t\\tMin\\tMax\\tMedia\")\n",
        "print (\"Jaccard:\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format( Jmin, Jmax, Jmean ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHs0znJAQ-s3",
        "outputId": "c3539f2d-ab0a-44b7-ef12-a3b9008b1c0a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\tMin\tMax\tMedia\n",
            "Jaccard:\t1.33\t6.00\t2.49\n"
          ]
        }
      ]
    }
  ]
}